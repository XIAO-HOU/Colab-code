{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "provenance": [],
      "mount_file_id": "1N0BP_9UijErqLHFlDTkQPfTr-TSNGYgI",
      "authorship_tag": "ABX9TyNsOyFVRme/swRcGPUtNKeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XIAO-HOU/Colab-code/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7g-kqOpFePw"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import defaultdict\n",
        "from torch import nn\n",
        "\n",
        "cora_path = 'drive/MyDrive/Colab Notebooks/data/cora'"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czzcSWJML1ct"
      },
      "source": [
        "def load_data(source):\n",
        "  content_path = source + '/cora.content'\n",
        "  cite_path = source + '/cora.cites'\n",
        "\n",
        "  features = []\n",
        "  labels = []\n",
        "  node_map = {}\n",
        "  label_map = {}\n",
        "\n",
        "  with open(content_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "      info = line.strip().split()\n",
        "      features.append([float(x) for x in info[1:-1]])\n",
        "      node_map[info[0]] = i\n",
        "      if info[-1] not in label_map:\n",
        "        label_map[info[-1]] = len(label_map)\n",
        "      labels.append(label_map[info[-1]])\n",
        "  features = np.asarray(features)\n",
        "  labels = np.asarray(labels)\n",
        "\n",
        "  adj_list = defaultdict(set)\n",
        "  adj_matrix = np.zeros((features.shape[0], features.shape[0]))\n",
        "  with open(cite_path) as f:\n",
        "    for i, line in enumerate(f):\n",
        "      info = line.strip().split()\n",
        "      assert len(info) == 2\n",
        "      paper1 = node_map[info[0]]\n",
        "      paper2 = node_map[info[1]]\n",
        "      adj_list[paper1].add(paper2)\n",
        "      adj_matrix[paper1][paper2] = 1\n",
        "      adj_list[paper2].add(paper1)\n",
        "      adj_matrix[paper2][paper1] = 1\n",
        "  \n",
        "  return features, labels, adj_list, adj_matrix\n",
        "\n",
        "features, labels, adj_list, adj_matrix = load_data(cora_path)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usjzI1YpGBwq"
      },
      "source": [
        "class GCN(nn.Module):\n",
        "  def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "      super(GCN, self).__init__()\n",
        "\n",
        "      self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "      self.gc2 = GraphConvolution(nhid, nclass)\n",
        "      self.dropout = dropout\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "      x = F.relu(self.gc1(x, adj))\n",
        "      x = F.dropout(x, self.dropout, training=self.training)\n",
        "      x = self.gc2(x, adj)\n",
        "      return F.log_softmax(x, dim=1)\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "  def __init__(self, in_features, out_features, bias = True):\n",
        "    super(GraphConvolution, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
        "    if bias:\n",
        "      self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
        "    else:\n",
        "      self.register_parameter('bias', None)\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "    self.weight.data.uniform_(-stdv, stdv)\n",
        "    if self.bias is not None:\n",
        "        self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "  def forward(self, input, adj):\n",
        "      support = torch.mm(input, self.weight)\n",
        "      output = torch.mm(adj, support)\n",
        "      if self.bias is not None:\n",
        "          return output + self.bias\n",
        "      else:\n",
        "          return output"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVsfanK0KyYf"
      },
      "source": [
        "def normalize(mx):\n",
        "  row_sum = np.array(mx.sum(1))\n",
        "  r_inv = np.power(row_sum, -0.5).flatten()\n",
        "  r_inv[np.isinf(r_inv)] = 0.\n",
        "  r_mat_inv = np.diag(r_inv)\n",
        "  mx = r_mat_inv.dot(mx).dot(r_mat_inv)\n",
        "  return mx\n",
        "\n",
        "def accuracy(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels)\n",
        "  correct = preds.eq(labels).double()\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)\n",
        "\n",
        "def split_data(nodes_num, test_split=3, val_split=6):\n",
        "  rand_indices = np.random.permutation(nodes_num)\n",
        "\n",
        "  test_size = nodes_num // test_split\n",
        "  val_size = nodes_num // val_split\n",
        "  # train_size = nodes_num - test_size - val_size\n",
        "\n",
        "  test_indexes = rand_indices[:test_size]\n",
        "  val_indexes = rand_indices[test_size:(test_size + val_size)]\n",
        "  train_indexes = rand_indices[test_size + val_size:]\n",
        "\n",
        "  return train_indexes, val_indexes, test_indexes"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VWij9V6IyfN"
      },
      "source": [
        "features = torch.Tensor(features)\n",
        "labels = torch.LongTensor(labels)\n",
        "adj_matrix = normalize(adj_matrix + np.eye(adj_matrix.shape[0]))\n",
        "adj_matrix = torch.Tensor(adj_matrix)\n",
        "train_indexes, val_indexes, test_indexes = split_data(features.shape[0])"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgXeLQ0ySU7G"
      },
      "source": [
        "hidden = 16\n",
        "dropout = 0.5\n",
        "lr = 0.01\n",
        "weight_decay = 5e-4\n",
        "\n",
        "model = GCN(nfeat=features.shape[1],\n",
        "        nhid=hidden,\n",
        "        nclass=labels.max().item() + 1,\n",
        "        dropout=dropout)\n",
        "optimizer = optim.Adam(model.parameters(),lr=lr,\n",
        "              weight_decay=weight_decay)\n",
        "\n",
        "def train(epoch):\n",
        "  t = time.time()\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(features, adj_matrix)\n",
        "  loss_train = F.nll_loss(output[train_indexes], labels[train_indexes])\n",
        "  acc_train = accuracy(output[train_indexes], labels[train_indexes])\n",
        "  loss_train.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  output = model(features, adj_matrix)\n",
        "\n",
        "  loss_val = F.nll_loss(output[val_indexes], labels[val_indexes])\n",
        "  acc_val = accuracy(output[val_indexes], labels[val_indexes])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "      'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "      'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "      'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "      'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "      'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "\n",
        "def test():\n",
        "  model.eval()\n",
        "  output = model(features, adj_matrix)\n",
        "  loss_test = F.nll_loss(output[test_indexes], labels[test_indexes])\n",
        "  acc_test = accuracy(output[test_indexes], labels[test_indexes])\n",
        "  print('Test set results:',\n",
        "      'loss={:.4f}'.format(loss_test.item()),\n",
        "      'accuracy={:.4f}'.format(acc_test.item()))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px0Kdrw4WwWv",
        "outputId": "fd6102d2-2bcf-49d6-fb18-ccfcb8de1568"
      },
      "source": [
        "epochs = 200\n",
        "start_time = time.time()\n",
        "for epoch in range(epochs):\n",
        "    train(epoch)\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - start_time))\n",
        "\n",
        "test()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9571 acc_train: 0.0952 loss_val: 1.8562 acc_val: 0.2106 time: 0.0705s\n",
            "Epoch: 0002 loss_train: 1.8820 acc_train: 0.1956 loss_val: 1.7875 acc_val: 0.3636 time: 0.0534s\n",
            "Epoch: 0003 loss_train: 1.8030 acc_train: 0.3100 loss_val: 1.7155 acc_val: 0.3525 time: 0.0498s\n",
            "Epoch: 0004 loss_train: 1.7247 acc_train: 0.3550 loss_val: 1.6364 acc_val: 0.3282 time: 0.0522s\n",
            "Epoch: 0005 loss_train: 1.6411 acc_train: 0.3572 loss_val: 1.5564 acc_val: 0.3082 time: 0.0553s\n",
            "Epoch: 0006 loss_train: 1.5667 acc_train: 0.3535 loss_val: 1.4793 acc_val: 0.3193 time: 0.0530s\n",
            "Epoch: 0007 loss_train: 1.4880 acc_train: 0.3697 loss_val: 1.4001 acc_val: 0.3525 time: 0.0503s\n",
            "Epoch: 0008 loss_train: 1.4173 acc_train: 0.4229 loss_val: 1.3163 acc_val: 0.4324 time: 0.0550s\n",
            "Epoch: 0009 loss_train: 1.3311 acc_train: 0.4657 loss_val: 1.2311 acc_val: 0.5743 time: 0.0539s\n",
            "Epoch: 0010 loss_train: 1.2401 acc_train: 0.5705 loss_val: 1.1503 acc_val: 0.6763 time: 0.0529s\n",
            "Epoch: 0011 loss_train: 1.1665 acc_train: 0.6354 loss_val: 1.0765 acc_val: 0.7428 time: 0.0577s\n",
            "Epoch: 0012 loss_train: 1.0973 acc_train: 0.6841 loss_val: 1.0072 acc_val: 0.7561 time: 0.0529s\n",
            "Epoch: 0013 loss_train: 1.0429 acc_train: 0.7188 loss_val: 0.9398 acc_val: 0.7761 time: 0.0557s\n",
            "Epoch: 0014 loss_train: 0.9715 acc_train: 0.7299 loss_val: 0.8744 acc_val: 0.7960 time: 0.0592s\n",
            "Epoch: 0015 loss_train: 0.8949 acc_train: 0.7528 loss_val: 0.8124 acc_val: 0.8071 time: 0.0598s\n",
            "Epoch: 0016 loss_train: 0.8492 acc_train: 0.7786 loss_val: 0.7556 acc_val: 0.8293 time: 0.0545s\n",
            "Epoch: 0017 loss_train: 0.7921 acc_train: 0.7779 loss_val: 0.7046 acc_val: 0.8426 time: 0.0524s\n",
            "Epoch: 0018 loss_train: 0.7534 acc_train: 0.8059 loss_val: 0.6583 acc_val: 0.8426 time: 0.0484s\n",
            "Epoch: 0019 loss_train: 0.7070 acc_train: 0.8170 loss_val: 0.6163 acc_val: 0.8537 time: 0.0528s\n",
            "Epoch: 0020 loss_train: 0.6638 acc_train: 0.8339 loss_val: 0.5782 acc_val: 0.8647 time: 0.0590s\n",
            "Epoch: 0021 loss_train: 0.6522 acc_train: 0.8303 loss_val: 0.5435 acc_val: 0.8670 time: 0.0554s\n",
            "Epoch: 0022 loss_train: 0.5861 acc_train: 0.8517 loss_val: 0.5124 acc_val: 0.8647 time: 0.0573s\n",
            "Epoch: 0023 loss_train: 0.5680 acc_train: 0.8443 loss_val: 0.4852 acc_val: 0.8736 time: 0.0598s\n",
            "Epoch: 0024 loss_train: 0.5474 acc_train: 0.8531 loss_val: 0.4616 acc_val: 0.8803 time: 0.0539s\n",
            "Epoch: 0025 loss_train: 0.5287 acc_train: 0.8731 loss_val: 0.4408 acc_val: 0.8825 time: 0.0545s\n",
            "Epoch: 0026 loss_train: 0.5074 acc_train: 0.8672 loss_val: 0.4227 acc_val: 0.8847 time: 0.0517s\n",
            "Epoch: 0027 loss_train: 0.4725 acc_train: 0.8768 loss_val: 0.4067 acc_val: 0.8847 time: 0.0518s\n",
            "Epoch: 0028 loss_train: 0.4661 acc_train: 0.8723 loss_val: 0.3931 acc_val: 0.8869 time: 0.0499s\n",
            "Epoch: 0029 loss_train: 0.4354 acc_train: 0.8790 loss_val: 0.3815 acc_val: 0.8891 time: 0.0564s\n",
            "Epoch: 0030 loss_train: 0.4136 acc_train: 0.8782 loss_val: 0.3719 acc_val: 0.8914 time: 0.0504s\n",
            "Epoch: 0031 loss_train: 0.4063 acc_train: 0.8856 loss_val: 0.3639 acc_val: 0.8891 time: 0.0532s\n",
            "Epoch: 0032 loss_train: 0.4026 acc_train: 0.8827 loss_val: 0.3568 acc_val: 0.8891 time: 0.0485s\n",
            "Epoch: 0033 loss_train: 0.3890 acc_train: 0.8915 loss_val: 0.3500 acc_val: 0.8891 time: 0.0542s\n",
            "Epoch: 0034 loss_train: 0.3925 acc_train: 0.8841 loss_val: 0.3438 acc_val: 0.8936 time: 0.0566s\n",
            "Epoch: 0035 loss_train: 0.3595 acc_train: 0.8930 loss_val: 0.3381 acc_val: 0.8936 time: 0.0512s\n",
            "Epoch: 0036 loss_train: 0.3588 acc_train: 0.8974 loss_val: 0.3334 acc_val: 0.9002 time: 0.0478s\n",
            "Epoch: 0037 loss_train: 0.3586 acc_train: 0.8923 loss_val: 0.3298 acc_val: 0.8980 time: 0.0526s\n",
            "Epoch: 0038 loss_train: 0.3227 acc_train: 0.9063 loss_val: 0.3266 acc_val: 0.8958 time: 0.0511s\n",
            "Epoch: 0039 loss_train: 0.3319 acc_train: 0.9107 loss_val: 0.3237 acc_val: 0.9002 time: 0.0498s\n",
            "Epoch: 0040 loss_train: 0.3264 acc_train: 0.9018 loss_val: 0.3212 acc_val: 0.9002 time: 0.0549s\n",
            "Epoch: 0041 loss_train: 0.3090 acc_train: 0.9173 loss_val: 0.3190 acc_val: 0.9002 time: 0.0540s\n",
            "Epoch: 0042 loss_train: 0.3182 acc_train: 0.9151 loss_val: 0.3173 acc_val: 0.9002 time: 0.0492s\n",
            "Epoch: 0043 loss_train: 0.3023 acc_train: 0.9173 loss_val: 0.3161 acc_val: 0.9002 time: 0.0503s\n",
            "Epoch: 0044 loss_train: 0.2991 acc_train: 0.9181 loss_val: 0.3155 acc_val: 0.9002 time: 0.0508s\n",
            "Epoch: 0045 loss_train: 0.2999 acc_train: 0.9026 loss_val: 0.3152 acc_val: 0.9002 time: 0.0573s\n",
            "Epoch: 0046 loss_train: 0.2941 acc_train: 0.9107 loss_val: 0.3153 acc_val: 0.8980 time: 0.0501s\n",
            "Epoch: 0047 loss_train: 0.2943 acc_train: 0.9166 loss_val: 0.3155 acc_val: 0.8958 time: 0.0487s\n",
            "Epoch: 0048 loss_train: 0.2837 acc_train: 0.9181 loss_val: 0.3155 acc_val: 0.8958 time: 0.0515s\n",
            "Epoch: 0049 loss_train: 0.2720 acc_train: 0.9188 loss_val: 0.3156 acc_val: 0.8936 time: 0.0510s\n",
            "Epoch: 0050 loss_train: 0.2690 acc_train: 0.9218 loss_val: 0.3158 acc_val: 0.8958 time: 0.0466s\n",
            "Epoch: 0051 loss_train: 0.2818 acc_train: 0.9247 loss_val: 0.3154 acc_val: 0.8958 time: 0.0526s\n",
            "Epoch: 0052 loss_train: 0.2704 acc_train: 0.9122 loss_val: 0.3148 acc_val: 0.8958 time: 0.0494s\n",
            "Epoch: 0053 loss_train: 0.2641 acc_train: 0.9188 loss_val: 0.3138 acc_val: 0.8980 time: 0.0565s\n",
            "Epoch: 0054 loss_train: 0.2603 acc_train: 0.9247 loss_val: 0.3136 acc_val: 0.8980 time: 0.0507s\n",
            "Epoch: 0055 loss_train: 0.2675 acc_train: 0.9225 loss_val: 0.3135 acc_val: 0.8980 time: 0.0457s\n",
            "Epoch: 0056 loss_train: 0.2610 acc_train: 0.9277 loss_val: 0.3138 acc_val: 0.8958 time: 0.0494s\n",
            "Epoch: 0057 loss_train: 0.2612 acc_train: 0.9328 loss_val: 0.3137 acc_val: 0.8958 time: 0.0569s\n",
            "Epoch: 0058 loss_train: 0.2571 acc_train: 0.9269 loss_val: 0.3140 acc_val: 0.8936 time: 0.0466s\n",
            "Epoch: 0059 loss_train: 0.2432 acc_train: 0.9269 loss_val: 0.3141 acc_val: 0.8869 time: 0.0499s\n",
            "Epoch: 0060 loss_train: 0.2475 acc_train: 0.9292 loss_val: 0.3145 acc_val: 0.8869 time: 0.0461s\n",
            "Epoch: 0061 loss_train: 0.2463 acc_train: 0.9358 loss_val: 0.3144 acc_val: 0.8891 time: 0.0461s\n",
            "Epoch: 0062 loss_train: 0.2503 acc_train: 0.9306 loss_val: 0.3146 acc_val: 0.8891 time: 0.0527s\n",
            "Epoch: 0063 loss_train: 0.2406 acc_train: 0.9328 loss_val: 0.3151 acc_val: 0.8914 time: 0.0490s\n",
            "Epoch: 0064 loss_train: 0.2382 acc_train: 0.9277 loss_val: 0.3158 acc_val: 0.8914 time: 0.0502s\n",
            "Epoch: 0065 loss_train: 0.2398 acc_train: 0.9321 loss_val: 0.3176 acc_val: 0.8914 time: 0.0470s\n",
            "Epoch: 0066 loss_train: 0.2431 acc_train: 0.9343 loss_val: 0.3189 acc_val: 0.8936 time: 0.0502s\n",
            "Epoch: 0067 loss_train: 0.2323 acc_train: 0.9306 loss_val: 0.3196 acc_val: 0.8936 time: 0.0487s\n",
            "Epoch: 0068 loss_train: 0.2241 acc_train: 0.9328 loss_val: 0.3198 acc_val: 0.8936 time: 0.0512s\n",
            "Epoch: 0069 loss_train: 0.2422 acc_train: 0.9314 loss_val: 0.3197 acc_val: 0.8914 time: 0.0518s\n",
            "Epoch: 0070 loss_train: 0.2337 acc_train: 0.9306 loss_val: 0.3195 acc_val: 0.8914 time: 0.0485s\n",
            "Epoch: 0071 loss_train: 0.2147 acc_train: 0.9358 loss_val: 0.3191 acc_val: 0.8914 time: 0.0520s\n",
            "Epoch: 0072 loss_train: 0.2233 acc_train: 0.9395 loss_val: 0.3180 acc_val: 0.8914 time: 0.0491s\n",
            "Epoch: 0073 loss_train: 0.2298 acc_train: 0.9432 loss_val: 0.3170 acc_val: 0.8914 time: 0.0483s\n",
            "Epoch: 0074 loss_train: 0.2158 acc_train: 0.9380 loss_val: 0.3167 acc_val: 0.8914 time: 0.0494s\n",
            "Epoch: 0075 loss_train: 0.2163 acc_train: 0.9410 loss_val: 0.3171 acc_val: 0.8914 time: 0.0548s\n",
            "Epoch: 0076 loss_train: 0.2249 acc_train: 0.9432 loss_val: 0.3183 acc_val: 0.8936 time: 0.0493s\n",
            "Epoch: 0077 loss_train: 0.2258 acc_train: 0.9410 loss_val: 0.3180 acc_val: 0.8936 time: 0.0459s\n",
            "Epoch: 0078 loss_train: 0.2170 acc_train: 0.9417 loss_val: 0.3178 acc_val: 0.8914 time: 0.0543s\n",
            "Epoch: 0079 loss_train: 0.2031 acc_train: 0.9439 loss_val: 0.3179 acc_val: 0.8936 time: 0.0555s\n",
            "Epoch: 0080 loss_train: 0.2193 acc_train: 0.9402 loss_val: 0.3191 acc_val: 0.8869 time: 0.0508s\n",
            "Epoch: 0081 loss_train: 0.2143 acc_train: 0.9439 loss_val: 0.3216 acc_val: 0.8891 time: 0.0515s\n",
            "Epoch: 0082 loss_train: 0.2094 acc_train: 0.9410 loss_val: 0.3245 acc_val: 0.8869 time: 0.0468s\n",
            "Epoch: 0083 loss_train: 0.2108 acc_train: 0.9424 loss_val: 0.3276 acc_val: 0.8869 time: 0.0496s\n",
            "Epoch: 0084 loss_train: 0.2070 acc_train: 0.9402 loss_val: 0.3286 acc_val: 0.8847 time: 0.0527s\n",
            "Epoch: 0085 loss_train: 0.2205 acc_train: 0.9410 loss_val: 0.3283 acc_val: 0.8869 time: 0.0498s\n",
            "Epoch: 0086 loss_train: 0.2059 acc_train: 0.9343 loss_val: 0.3266 acc_val: 0.8891 time: 0.0514s\n",
            "Epoch: 0087 loss_train: 0.2128 acc_train: 0.9387 loss_val: 0.3242 acc_val: 0.8914 time: 0.0507s\n",
            "Epoch: 0088 loss_train: 0.2010 acc_train: 0.9410 loss_val: 0.3204 acc_val: 0.8914 time: 0.0515s\n",
            "Epoch: 0089 loss_train: 0.2057 acc_train: 0.9417 loss_val: 0.3179 acc_val: 0.8914 time: 0.0507s\n",
            "Epoch: 0090 loss_train: 0.2101 acc_train: 0.9410 loss_val: 0.3170 acc_val: 0.8914 time: 0.0503s\n",
            "Epoch: 0091 loss_train: 0.2066 acc_train: 0.9402 loss_val: 0.3172 acc_val: 0.8914 time: 0.0516s\n",
            "Epoch: 0092 loss_train: 0.1961 acc_train: 0.9520 loss_val: 0.3191 acc_val: 0.8914 time: 0.0464s\n",
            "Epoch: 0093 loss_train: 0.1962 acc_train: 0.9410 loss_val: 0.3207 acc_val: 0.8914 time: 0.0574s\n",
            "Epoch: 0094 loss_train: 0.2029 acc_train: 0.9439 loss_val: 0.3221 acc_val: 0.8869 time: 0.0473s\n",
            "Epoch: 0095 loss_train: 0.1992 acc_train: 0.9439 loss_val: 0.3241 acc_val: 0.8869 time: 0.0492s\n",
            "Epoch: 0096 loss_train: 0.2030 acc_train: 0.9365 loss_val: 0.3246 acc_val: 0.8891 time: 0.0490s\n",
            "Epoch: 0097 loss_train: 0.1921 acc_train: 0.9506 loss_val: 0.3241 acc_val: 0.8891 time: 0.0503s\n",
            "Epoch: 0098 loss_train: 0.1978 acc_train: 0.9461 loss_val: 0.3226 acc_val: 0.8936 time: 0.0522s\n",
            "Epoch: 0099 loss_train: 0.2039 acc_train: 0.9432 loss_val: 0.3203 acc_val: 0.8936 time: 0.0497s\n",
            "Epoch: 0100 loss_train: 0.1932 acc_train: 0.9395 loss_val: 0.3191 acc_val: 0.8958 time: 0.0513s\n",
            "Epoch: 0101 loss_train: 0.1807 acc_train: 0.9520 loss_val: 0.3181 acc_val: 0.8958 time: 0.0472s\n",
            "Epoch: 0102 loss_train: 0.1802 acc_train: 0.9550 loss_val: 0.3174 acc_val: 0.8958 time: 0.0542s\n",
            "Epoch: 0103 loss_train: 0.2112 acc_train: 0.9380 loss_val: 0.3170 acc_val: 0.8980 time: 0.0512s\n",
            "Epoch: 0104 loss_train: 0.1908 acc_train: 0.9461 loss_val: 0.3185 acc_val: 0.8980 time: 0.0523s\n",
            "Epoch: 0105 loss_train: 0.1753 acc_train: 0.9542 loss_val: 0.3192 acc_val: 0.8980 time: 0.0517s\n",
            "Epoch: 0106 loss_train: 0.1883 acc_train: 0.9557 loss_val: 0.3206 acc_val: 0.8980 time: 0.0502s\n",
            "Epoch: 0107 loss_train: 0.1870 acc_train: 0.9461 loss_val: 0.3230 acc_val: 0.8980 time: 0.0482s\n",
            "Epoch: 0108 loss_train: 0.1848 acc_train: 0.9454 loss_val: 0.3245 acc_val: 0.8958 time: 0.0508s\n",
            "Epoch: 0109 loss_train: 0.1931 acc_train: 0.9424 loss_val: 0.3245 acc_val: 0.8958 time: 0.0497s\n",
            "Epoch: 0110 loss_train: 0.1802 acc_train: 0.9520 loss_val: 0.3236 acc_val: 0.8958 time: 0.0530s\n",
            "Epoch: 0111 loss_train: 0.1889 acc_train: 0.9469 loss_val: 0.3219 acc_val: 0.8914 time: 0.0524s\n",
            "Epoch: 0112 loss_train: 0.1836 acc_train: 0.9469 loss_val: 0.3212 acc_val: 0.8914 time: 0.0517s\n",
            "Epoch: 0113 loss_train: 0.1868 acc_train: 0.9483 loss_val: 0.3218 acc_val: 0.8936 time: 0.0521s\n",
            "Epoch: 0114 loss_train: 0.1895 acc_train: 0.9461 loss_val: 0.3234 acc_val: 0.8936 time: 0.0531s\n",
            "Epoch: 0115 loss_train: 0.1809 acc_train: 0.9528 loss_val: 0.3254 acc_val: 0.8936 time: 0.0518s\n",
            "Epoch: 0116 loss_train: 0.1879 acc_train: 0.9417 loss_val: 0.3253 acc_val: 0.8958 time: 0.0478s\n",
            "Epoch: 0117 loss_train: 0.1839 acc_train: 0.9528 loss_val: 0.3235 acc_val: 0.8958 time: 0.0468s\n",
            "Epoch: 0118 loss_train: 0.1805 acc_train: 0.9498 loss_val: 0.3214 acc_val: 0.8958 time: 0.0487s\n",
            "Epoch: 0119 loss_train: 0.1933 acc_train: 0.9498 loss_val: 0.3200 acc_val: 0.8936 time: 0.0539s\n",
            "Epoch: 0120 loss_train: 0.1884 acc_train: 0.9417 loss_val: 0.3189 acc_val: 0.8936 time: 0.0528s\n",
            "Epoch: 0121 loss_train: 0.1726 acc_train: 0.9579 loss_val: 0.3199 acc_val: 0.8914 time: 0.0524s\n",
            "Epoch: 0122 loss_train: 0.1752 acc_train: 0.9446 loss_val: 0.3221 acc_val: 0.8891 time: 0.0538s\n",
            "Epoch: 0123 loss_train: 0.1834 acc_train: 0.9535 loss_val: 0.3234 acc_val: 0.8914 time: 0.0539s\n",
            "Epoch: 0124 loss_train: 0.1713 acc_train: 0.9550 loss_val: 0.3252 acc_val: 0.8869 time: 0.0475s\n",
            "Epoch: 0125 loss_train: 0.1824 acc_train: 0.9520 loss_val: 0.3262 acc_val: 0.8869 time: 0.0505s\n",
            "Epoch: 0126 loss_train: 0.1714 acc_train: 0.9528 loss_val: 0.3260 acc_val: 0.8869 time: 0.0479s\n",
            "Epoch: 0127 loss_train: 0.1801 acc_train: 0.9483 loss_val: 0.3258 acc_val: 0.8847 time: 0.0531s\n",
            "Epoch: 0128 loss_train: 0.1745 acc_train: 0.9483 loss_val: 0.3237 acc_val: 0.8847 time: 0.0499s\n",
            "Epoch: 0129 loss_train: 0.1612 acc_train: 0.9587 loss_val: 0.3211 acc_val: 0.8847 time: 0.0518s\n",
            "Epoch: 0130 loss_train: 0.1822 acc_train: 0.9491 loss_val: 0.3185 acc_val: 0.8869 time: 0.0498s\n",
            "Epoch: 0131 loss_train: 0.1819 acc_train: 0.9520 loss_val: 0.3179 acc_val: 0.8847 time: 0.0479s\n",
            "Epoch: 0132 loss_train: 0.1799 acc_train: 0.9491 loss_val: 0.3182 acc_val: 0.8847 time: 0.0556s\n",
            "Epoch: 0133 loss_train: 0.1627 acc_train: 0.9557 loss_val: 0.3186 acc_val: 0.8847 time: 0.0504s\n",
            "Epoch: 0134 loss_train: 0.1711 acc_train: 0.9506 loss_val: 0.3199 acc_val: 0.8869 time: 0.0492s\n",
            "Epoch: 0135 loss_train: 0.1703 acc_train: 0.9624 loss_val: 0.3226 acc_val: 0.8869 time: 0.0506s\n",
            "Epoch: 0136 loss_train: 0.1616 acc_train: 0.9587 loss_val: 0.3246 acc_val: 0.8869 time: 0.0528s\n",
            "Epoch: 0137 loss_train: 0.1745 acc_train: 0.9506 loss_val: 0.3251 acc_val: 0.8936 time: 0.0490s\n",
            "Epoch: 0138 loss_train: 0.1740 acc_train: 0.9535 loss_val: 0.3260 acc_val: 0.8936 time: 0.0553s\n",
            "Epoch: 0139 loss_train: 0.1665 acc_train: 0.9498 loss_val: 0.3278 acc_val: 0.8936 time: 0.0497s\n",
            "Epoch: 0140 loss_train: 0.1657 acc_train: 0.9542 loss_val: 0.3293 acc_val: 0.8936 time: 0.0549s\n",
            "Epoch: 0141 loss_train: 0.1603 acc_train: 0.9528 loss_val: 0.3305 acc_val: 0.8914 time: 0.0483s\n",
            "Epoch: 0142 loss_train: 0.1539 acc_train: 0.9579 loss_val: 0.3311 acc_val: 0.8891 time: 0.0482s\n",
            "Epoch: 0143 loss_train: 0.1588 acc_train: 0.9542 loss_val: 0.3311 acc_val: 0.8958 time: 0.0513s\n",
            "Epoch: 0144 loss_train: 0.1585 acc_train: 0.9542 loss_val: 0.3295 acc_val: 0.8936 time: 0.0466s\n",
            "Epoch: 0145 loss_train: 0.1614 acc_train: 0.9579 loss_val: 0.3277 acc_val: 0.8914 time: 0.0510s\n",
            "Epoch: 0146 loss_train: 0.1684 acc_train: 0.9528 loss_val: 0.3267 acc_val: 0.8891 time: 0.0515s\n",
            "Epoch: 0147 loss_train: 0.1577 acc_train: 0.9572 loss_val: 0.3261 acc_val: 0.8891 time: 0.0498s\n",
            "Epoch: 0148 loss_train: 0.1621 acc_train: 0.9565 loss_val: 0.3248 acc_val: 0.8914 time: 0.0497s\n",
            "Epoch: 0149 loss_train: 0.1568 acc_train: 0.9557 loss_val: 0.3251 acc_val: 0.8914 time: 0.0552s\n",
            "Epoch: 0150 loss_train: 0.1572 acc_train: 0.9557 loss_val: 0.3273 acc_val: 0.8869 time: 0.0505s\n",
            "Epoch: 0151 loss_train: 0.1629 acc_train: 0.9542 loss_val: 0.3292 acc_val: 0.8891 time: 0.0475s\n",
            "Epoch: 0152 loss_train: 0.1651 acc_train: 0.9506 loss_val: 0.3302 acc_val: 0.8891 time: 0.0543s\n",
            "Epoch: 0153 loss_train: 0.1643 acc_train: 0.9542 loss_val: 0.3323 acc_val: 0.8891 time: 0.0499s\n",
            "Epoch: 0154 loss_train: 0.1594 acc_train: 0.9535 loss_val: 0.3334 acc_val: 0.8891 time: 0.0590s\n",
            "Epoch: 0155 loss_train: 0.1480 acc_train: 0.9616 loss_val: 0.3326 acc_val: 0.8891 time: 0.0503s\n",
            "Epoch: 0156 loss_train: 0.1708 acc_train: 0.9520 loss_val: 0.3313 acc_val: 0.8891 time: 0.0498s\n",
            "Epoch: 0157 loss_train: 0.1447 acc_train: 0.9594 loss_val: 0.3300 acc_val: 0.8914 time: 0.0510s\n",
            "Epoch: 0158 loss_train: 0.1578 acc_train: 0.9557 loss_val: 0.3286 acc_val: 0.8891 time: 0.0537s\n",
            "Epoch: 0159 loss_train: 0.1562 acc_train: 0.9520 loss_val: 0.3276 acc_val: 0.8869 time: 0.0518s\n",
            "Epoch: 0160 loss_train: 0.1628 acc_train: 0.9528 loss_val: 0.3279 acc_val: 0.8869 time: 0.0507s\n",
            "Epoch: 0161 loss_train: 0.1631 acc_train: 0.9542 loss_val: 0.3308 acc_val: 0.8825 time: 0.0541s\n",
            "Epoch: 0162 loss_train: 0.1580 acc_train: 0.9506 loss_val: 0.3332 acc_val: 0.8803 time: 0.0529s\n",
            "Epoch: 0163 loss_train: 0.1577 acc_train: 0.9542 loss_val: 0.3337 acc_val: 0.8803 time: 0.0494s\n",
            "Epoch: 0164 loss_train: 0.1506 acc_train: 0.9565 loss_val: 0.3329 acc_val: 0.8825 time: 0.0495s\n",
            "Epoch: 0165 loss_train: 0.1460 acc_train: 0.9565 loss_val: 0.3324 acc_val: 0.8825 time: 0.0509s\n",
            "Epoch: 0166 loss_train: 0.1558 acc_train: 0.9513 loss_val: 0.3332 acc_val: 0.8825 time: 0.0504s\n",
            "Epoch: 0167 loss_train: 0.1538 acc_train: 0.9542 loss_val: 0.3348 acc_val: 0.8803 time: 0.0544s\n",
            "Epoch: 0168 loss_train: 0.1491 acc_train: 0.9572 loss_val: 0.3370 acc_val: 0.8780 time: 0.0514s\n",
            "Epoch: 0169 loss_train: 0.1573 acc_train: 0.9587 loss_val: 0.3373 acc_val: 0.8803 time: 0.0501s\n",
            "Epoch: 0170 loss_train: 0.1484 acc_train: 0.9601 loss_val: 0.3386 acc_val: 0.8847 time: 0.0494s\n",
            "Epoch: 0171 loss_train: 0.1410 acc_train: 0.9565 loss_val: 0.3387 acc_val: 0.8847 time: 0.0520s\n",
            "Epoch: 0172 loss_train: 0.1538 acc_train: 0.9535 loss_val: 0.3361 acc_val: 0.8847 time: 0.0485s\n",
            "Epoch: 0173 loss_train: 0.1495 acc_train: 0.9587 loss_val: 0.3336 acc_val: 0.8869 time: 0.0526s\n",
            "Epoch: 0174 loss_train: 0.1579 acc_train: 0.9565 loss_val: 0.3325 acc_val: 0.8891 time: 0.0544s\n",
            "Epoch: 0175 loss_train: 0.1498 acc_train: 0.9542 loss_val: 0.3323 acc_val: 0.8869 time: 0.0526s\n",
            "Epoch: 0176 loss_train: 0.1518 acc_train: 0.9528 loss_val: 0.3315 acc_val: 0.8869 time: 0.0532s\n",
            "Epoch: 0177 loss_train: 0.1540 acc_train: 0.9565 loss_val: 0.3325 acc_val: 0.8869 time: 0.0521s\n",
            "Epoch: 0178 loss_train: 0.1533 acc_train: 0.9579 loss_val: 0.3332 acc_val: 0.8914 time: 0.0569s\n",
            "Epoch: 0179 loss_train: 0.1563 acc_train: 0.9528 loss_val: 0.3347 acc_val: 0.8914 time: 0.0517s\n",
            "Epoch: 0180 loss_train: 0.1516 acc_train: 0.9557 loss_val: 0.3356 acc_val: 0.8914 time: 0.0503s\n",
            "Epoch: 0181 loss_train: 0.1539 acc_train: 0.9550 loss_val: 0.3356 acc_val: 0.8891 time: 0.0517s\n",
            "Epoch: 0182 loss_train: 0.1566 acc_train: 0.9587 loss_val: 0.3367 acc_val: 0.8847 time: 0.0541s\n",
            "Epoch: 0183 loss_train: 0.1440 acc_train: 0.9675 loss_val: 0.3399 acc_val: 0.8847 time: 0.0503s\n",
            "Epoch: 0184 loss_train: 0.1491 acc_train: 0.9624 loss_val: 0.3432 acc_val: 0.8825 time: 0.0503s\n",
            "Epoch: 0185 loss_train: 0.1416 acc_train: 0.9601 loss_val: 0.3460 acc_val: 0.8803 time: 0.0495s\n",
            "Epoch: 0186 loss_train: 0.1431 acc_train: 0.9520 loss_val: 0.3475 acc_val: 0.8825 time: 0.0506s\n",
            "Epoch: 0187 loss_train: 0.1438 acc_train: 0.9535 loss_val: 0.3487 acc_val: 0.8825 time: 0.0504s\n",
            "Epoch: 0188 loss_train: 0.1459 acc_train: 0.9565 loss_val: 0.3464 acc_val: 0.8825 time: 0.0475s\n",
            "Epoch: 0189 loss_train: 0.1476 acc_train: 0.9565 loss_val: 0.3426 acc_val: 0.8825 time: 0.0501s\n",
            "Epoch: 0190 loss_train: 0.1397 acc_train: 0.9565 loss_val: 0.3414 acc_val: 0.8825 time: 0.0578s\n",
            "Epoch: 0191 loss_train: 0.1512 acc_train: 0.9535 loss_val: 0.3415 acc_val: 0.8825 time: 0.0487s\n",
            "Epoch: 0192 loss_train: 0.1404 acc_train: 0.9638 loss_val: 0.3425 acc_val: 0.8803 time: 0.0533s\n",
            "Epoch: 0193 loss_train: 0.1463 acc_train: 0.9587 loss_val: 0.3424 acc_val: 0.8847 time: 0.0507s\n",
            "Epoch: 0194 loss_train: 0.1425 acc_train: 0.9624 loss_val: 0.3418 acc_val: 0.8847 time: 0.0556s\n",
            "Epoch: 0195 loss_train: 0.1481 acc_train: 0.9616 loss_val: 0.3389 acc_val: 0.8869 time: 0.0509s\n",
            "Epoch: 0196 loss_train: 0.1470 acc_train: 0.9638 loss_val: 0.3364 acc_val: 0.8891 time: 0.0582s\n",
            "Epoch: 0197 loss_train: 0.1506 acc_train: 0.9565 loss_val: 0.3370 acc_val: 0.8869 time: 0.0515s\n",
            "Epoch: 0198 loss_train: 0.1424 acc_train: 0.9594 loss_val: 0.3379 acc_val: 0.8869 time: 0.0558s\n",
            "Epoch: 0199 loss_train: 0.1497 acc_train: 0.9557 loss_val: 0.3395 acc_val: 0.8847 time: 0.0538s\n",
            "Epoch: 0200 loss_train: 0.1400 acc_train: 0.9616 loss_val: 0.3390 acc_val: 0.8891 time: 0.0534s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 10.5197s\n",
            "Test set results: loss=0.4745 accuracy=0.8603\n"
          ]
        }
      ]
    }
  ]
}